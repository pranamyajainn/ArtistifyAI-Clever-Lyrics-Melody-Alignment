{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6afdf89-0b94-48b8-aad8-c6a99b74bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow is now using: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set TensorFlow to use only one GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')  # Use GPU 0 only\n",
    "        print(f\"✅ TensorFlow is now using: {tf.config.list_physical_devices('GPU')}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ Runtime Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842218b6-b2c2-47e3-892b-b68935e7831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow is now using: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Disable virtual devices by resetting logical configurations\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]\n",
    "        )\n",
    "        \n",
    "        # Do NOT set memory growth, just confirm the GPUs\n",
    "        print(f\"✅ TensorFlow is now using: {tf.config.list_physical_devices('GPU')}\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ Runtime Error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No GPU found! TensorFlow is running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bfab92-b45e-4731-be2a-fb689a573810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69b98ca-4cde-4816-9228-7a41860b446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c88039a-1d68-4606-836b-07d508f45dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleared GPU memory!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numba import cuda\n",
    "\n",
    "# Reset TensorFlow sessions\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Manually clear GPU memory\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "\n",
    "cuda.select_device(1)\n",
    "cuda.close()\n",
    "\n",
    "print(\"✅ Cleared GPU memory!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c0e226-c3b8-45aa-9d9c-8d8773f57193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92b132e-a1cc-46a5-bb10-4a4f29fb15fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU memory usage limited to prevent crashes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Limit GPU memory usage\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3000)]  # Adjust if needed\n",
    "            )\n",
    "        print(\"✅ GPU memory usage limited to prevent crashes.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ Error setting memory limit: {e}\")\n",
    "else:\n",
    "    print(\"❌ No GPU found! TensorFlow is running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b147e09-4e4e-46a2-98f3-340151392f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available memory: 16.79 GB\n",
      "Loading minimal data...\n",
      "Total unique characters: 52\n",
      "Creating minimal model...\n",
      "Error occurred: Memory growth cannot differ between GPU devices\n",
      "\n",
      "If your kernel is still crashing, you need to try these steps:\n",
      "1. Restart your Jupyter kernel and run only this code\n",
      "2. Try running on Google Colab instead of locally\n",
      "3. Train your model using a script outside of Jupyter\n",
      "4. Reduce the size of your cleaned_lyrics.csv file before loading\n",
      "\n",
      "Trying absolute minimal version as last resort...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Memory growth cannot differ between GPU devices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 157\u001b[0m\n\u001b[0;32m    154\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chars)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Ultra-minimal model that should run anywhere\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m tiny_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSimpleRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m tiny_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    163\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    164\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m )\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Create minimal training data (just 5 examples)\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\artistify\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\artistify\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\artistify\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1261\u001b[0m, in \u001b[0;36mContext._compute_gpu_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m virtual_devices \u001b[38;5;129;01mand\u001b[39;00m memory_growths:\n\u001b[0;32m   1260\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory_growths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory growth cannot differ between GPU devices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1262\u001b[0m   allow_growth \u001b[38;5;241m=\u001b[39m memory_growths\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Memory growth cannot differ between GPU devices"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Memory-saving settings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TensorFlow logging\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True) if tf.config.list_physical_devices('GPU') else None\n",
    "\n",
    "# Check available memory and alert user\n",
    "def check_memory():\n",
    "    try:\n",
    "        import psutil\n",
    "        available_memory_gb = psutil.virtual_memory().available / (1024 ** 3)\n",
    "        print(f\"Available memory: {available_memory_gb:.2f} GB\")\n",
    "        if available_memory_gb < 2:\n",
    "            print(\"WARNING: Very low memory available. This might still fail.\")\n",
    "    except ImportError:\n",
    "        print(\"psutil not installed - can't check available memory\")\n",
    "\n",
    "check_memory()\n",
    "\n",
    "# Character-level model requires much less memory than word-level\n",
    "print(\"Loading minimal data...\")\n",
    "try:\n",
    "    # Only load a small subset of the data\n",
    "    df = pd.read_csv(\"../data/cleaned_lyrics.csv\", nrows=20)  # Just 20 songs\n",
    "    \n",
    "    # Concatenate with enough spacing\n",
    "    text = \"\\n\\n\".join(df['clean_lyrics'].str[:300])  # First 300 chars of each song\n",
    "    \n",
    "    # Create character mapping (much smaller than word mapping)\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "    idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "    \n",
    "    vocab_size = len(chars)\n",
    "    print(f\"Total unique characters: {vocab_size}\")\n",
    "    \n",
    "    # Use a custom data generator to avoid loading all sequences into memory\n",
    "    class DataGenerator(tf.keras.utils.Sequence):\n",
    "        def __init__(self, text, char_to_idx, seq_length=40, batch_size=64):\n",
    "            self.text = text\n",
    "            self.char_to_idx = char_to_idx\n",
    "            self.seq_length = seq_length\n",
    "            self.batch_size = batch_size\n",
    "            # Calculate valid starting positions (leaving room for a sequence)\n",
    "            self.indices = list(range(len(text) - seq_length))\n",
    "            \n",
    "        def __len__(self):\n",
    "            # Return number of batches\n",
    "            return max(1, len(self.indices) // self.batch_size)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            # Generate one batch\n",
    "            batch_indices = self.indices[idx * self.batch_size:\n",
    "                                        (idx + 1) * self.batch_size]\n",
    "            \n",
    "            # Prepare batch data\n",
    "            X = np.zeros((len(batch_indices), self.seq_length, vocab_size), dtype=np.bool_)\n",
    "            y = np.zeros((len(batch_indices), vocab_size), dtype=np.bool_)\n",
    "            \n",
    "            for i, start_idx in enumerate(batch_indices):\n",
    "                # Get sequence and target\n",
    "                seq = text[start_idx:start_idx + self.seq_length]\n",
    "                target = text[start_idx + self.seq_length]\n",
    "                \n",
    "                # One-hot encode sequence\n",
    "                for t, char in enumerate(seq):\n",
    "                    X[i, t, char_to_idx[char]] = 1\n",
    "                \n",
    "                # One-hot encode target\n",
    "                y[i, char_to_idx[target]] = 1\n",
    "                \n",
    "            return X, y\n",
    "    \n",
    "    # Ultra-minimal hyperparameters\n",
    "    seq_length = 40  # Sequence length\n",
    "    batch_size = 32  # Small batch size\n",
    "    \n",
    "    # Create data generator\n",
    "    data_gen = DataGenerator(text, char_to_idx, seq_length, batch_size)\n",
    "    \n",
    "    # Create a very tiny model\n",
    "    print(\"Creating minimal model...\")\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, input_shape=(seq_length, vocab_size)),\n",
    "        tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile with minimal settings\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss='categorical_crossentropy'\n",
    "    )\n",
    "    \n",
    "    # Train for just a few epochs\n",
    "    print(\"Training minimal model...\")\n",
    "    model.fit(\n",
    "        data_gen,\n",
    "        epochs=5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Generate a small sample\n",
    "    print(\"\\nGenerating sample text:\")\n",
    "    \n",
    "    def generate_text(model, start_text, char_to_idx, idx_to_char, length=100):\n",
    "        text = start_text\n",
    "        for _ in range(length):\n",
    "            # One-hot encode the last seq_length characters\n",
    "            x = np.zeros((1, seq_length, vocab_size))\n",
    "            for t, char in enumerate(text[-seq_length:]):\n",
    "                if char in char_to_idx:\n",
    "                    x[0, t, char_to_idx[char]] = 1\n",
    "            \n",
    "            # Predict next character\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_idx = np.random.choice(len(preds), p=preds)\n",
    "            next_char = idx_to_char[next_idx]\n",
    "            \n",
    "            # Add to text\n",
    "            text += next_char\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    # Save a very small model\n",
    "    model.save(\"../models/char_lyrics_generator_tiny.h5\")\n",
    "    print(\"✅ Character-level model saved successfully\")\n",
    "    \n",
    "    # Generate small sample\n",
    "    if len(text) > seq_length:\n",
    "        sample = generate_text(model, text[:seq_length], char_to_idx, idx_to_char, 100)\n",
    "        print(sample)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(\"\\nIf your kernel is still crashing, you need to try these steps:\")\n",
    "    print(\"1. Restart your Jupyter kernel and run only this code\")\n",
    "    print(\"2. Try running on Google Colab instead of locally\")\n",
    "    print(\"3. Train your model using a script outside of Jupyter\")\n",
    "    print(\"4. Reduce the size of your cleaned_lyrics.csv file before loading\")\n",
    "    \n",
    "    # If still failing, try running this minimal version\n",
    "    print(\"\\nTrying absolute minimal version as last resort...\")\n",
    "    \n",
    "    # Create fake data if we couldn't load the real data\n",
    "    if 'df' not in locals():\n",
    "        text = \"This is a sample lyric. I love to sing. Music is great.\"\n",
    "        chars = sorted(list(set(text)))\n",
    "        char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "        idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "        vocab_size = len(chars)\n",
    "    \n",
    "    # Ultra-minimal model that should run anywhere\n",
    "    tiny_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(16, input_shape=(10, vocab_size)),\n",
    "        tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    tiny_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy'\n",
    "    )\n",
    "    \n",
    "    # Create minimal training data (just 5 examples)\n",
    "    X = np.zeros((5, 10, vocab_size))\n",
    "    y = np.zeros((5, vocab_size))\n",
    "    \n",
    "    # Just train on a single batch\n",
    "    tiny_model.fit(X, y, epochs=1, verbose=1)\n",
    "    \n",
    "    tiny_model.save(\"../models/emergency_minimal_model.h5\")\n",
    "    print(\"Emergency minimal model saved - this is just a placeholder!\")\n",
    "    print(\"You'll need to train a real model in a different environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d876ef-73ed-4caa-8f5e-558d1cc3476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Loss', color='red')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy Curve (optional — accuracy isn’t too meaningful for text generation, but still useful)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Accuracy', color='green')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fc0f1-0642-4041-8055-b240488f6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save tokenizer to file\n",
    "with open(\"../models/tokenizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "print(\"✅ Tokenizer saved to ../models/tokenizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64180d80-557e-4524-a22b-df8e443e3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('../models/zayn_lyrics_generator.h5')\n",
    "\n",
    "# Load the tokenizer (make sure you saved it during training)\n",
    "import pickle\n",
    "with open('../models/tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "\n",
    "# Define a function to generate lyrics\n",
    "def generate_lyrics(seed_text, next_words=50):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = np.array(token_list).reshape(1, -1)\n",
    "\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "# Test it with a starting seed\n",
    "print(generate_lyrics(\"I can feel\", next_words=50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35e8c0-280f-4b7f-9d5c-269bf5dcf647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e36d0-2d88-47bb-a1ac-4df2d7cdf47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73f09e-f587-4d27-a356-7abc4c1c4b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421be52-2057-4e9a-b4fb-2dcac28f9fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9fcd9-ac34-4342-9615-3a49fa3b5e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
